{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_Workshop_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinojossy93/my_stuff/blob/master/PySpark_Workshop_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7HoPfAO6z_",
        "colab_type": "text"
      },
      "source": [
        "# ETL made simple with PySpark. A beginner's guide. Part 1.\n",
        "\n",
        "Created by: Aida Martinez\n",
        "\n",
        "In this workshop, we will create an ETL to extract crime data from different cities, transform it, and load it into a file. The datasets reflects reported incidents of crime that occurred in the Cities of San Francisco, Austin and Chicago during 2018. See the references at the end of this notebook for more information.\n",
        "\n",
        "What will be covering?\n",
        "- Installing and setting up PySpark\n",
        "- Data Extraction\n",
        "  - Read from Data Sources\n",
        "  - Apply Schemas\n",
        "  - Handling corrupt records\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbetLOWze8qR",
        "colab_type": "text"
      },
      "source": [
        "# Installing and Setting Up Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47kjxWKJ5cuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_WT90if5i_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up the environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbvDlovA08G8",
        "colab_type": "text"
      },
      "source": [
        "PySpark isn't on sys.path by default, so that's why we use *finspark* to add it to sys.path at runtime. You need to import and call the *init()* method of findspark before calling PySpark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QvnCMGAeDo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K3Gp1Q63gZ3-"
      },
      "source": [
        "For every Spark application, the first operation is to connect to the Spark master and get a\n",
        "Spark session. This is an operation you will do every time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Hewu7ggQQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Getting a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa2zc43aqLxX",
        "colab_type": "text"
      },
      "source": [
        "Until now, we connected the Spark application (or driver) to the master and we got a Spark session. SparkSession is the new entry point for Spark applications starting on **Apache Spark 2.0**. Spark Session allows you to interact with underlying Spark functionality and programming Spark with DataFrame and Dataset APIs (Scala)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9b8M6qkO5sM",
        "colab_type": "text"
      },
      "source": [
        "In order to get and load the data into our local machine we use the `wget` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKyqTZMbmCA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get San Francisco Crime data\n",
        "!wget -cq https://datasets-pyspark-workshop.s3.amazonaws.com/San_Francisco_Crime_2018.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dDIPfz5u1uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Chicago crime data\n",
        "!wget -cq https://datasets-pyspark-workshop.s3.amazonaws.com/Chicago_Crimes_2018.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDSrg9ujvEdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Austin crime data\n",
        "!wget -cq https://datasets-pyspark-workshop.s3.amazonaws.com/Austin_Crime_Reports_2018.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx3YrdDjk-Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get San Francisco corrupted data\n",
        "!wget -cq https://datasets-pyspark-workshop.s3.amazonaws.com/San_Francisco_Crime_2018_corrupt.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_-AkFrg9kat",
        "colab_type": "text"
      },
      "source": [
        "# DataFrames\n",
        "A dataframe is both a data structure and an API. Once created (instantiated), a DataFrame object has methods attached to it. Methods are operations one can perform on DataFrames such as filtering, counting, aggregating and many others.\n",
        "\n",
        "Dataframes, as well as datasets and RDDs (resilient distributed datasets), are considered immutable storage. When applied a method to an DataFrame, its state cannot be modified after it is created.\n",
        "\n",
        "The `.` indicates you are *applying a method on the object*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb7yR0QYyjlX",
        "colab_type": "text"
      },
      "source": [
        "# Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ZP_1AVMH6j",
        "colab_type": "text"
      },
      "source": [
        "## Data Sources\n",
        "Spark has six **core** data sources and hundreds of external data sources written by the community. Following are Spark’s core data\n",
        "sources:\n",
        "- [CSV](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
        "- [JSON](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json)\n",
        "- [Parquet](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet)\n",
        "- [ORC](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.orc)\n",
        "- [JDBC/ODBC](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc) connections\n",
        "- [Plain-text](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.text) files\n",
        "\n",
        "Some of the community-created data sources are:\n",
        "- [Cassandra](https://docs.databricks.com/spark/latest/data-sources/cassandra.html)\n",
        "- [MongoDB](https://docs.databricks.com/spark/latest/data-sources/mongodb.html)\n",
        "- [AWS Redshift](https://docs.databricks.com/spark/latest/data-sources/aws/amazon-redshift.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-fAic8_8daa",
        "colab_type": "text"
      },
      "source": [
        "## The Design Pattern\n",
        "\n",
        "All connections to datasources work in much the same way, whether your data sits in S3, Cassandra, Redshift, Relational DB or another common data store.  The general pattern is always: \n",
        " \n",
        " ```\n",
        " spark.read.options(<option key>, <option value>).<connection_type>(<endpoint>)\n",
        " ```.\n",
        "\n",
        "The `connection_type` can be `csv`, `jdbc`, `parquet`, `json`, etc.\n",
        "\n",
        "Let's start by reading the first file and saving it into a Spark DataFrame. As all good masters, the cluster manager doesn't do much, it relies on slaves or workers to do the job. \n",
        "Spark will do a distributed ingestion which means you will ask `n` workers to ingest the file at the same time.\n",
        "The workers will create tasks to read the file. Each worker has access to the node’s memory and will assign a memory partition to the task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpJZ6dBrKwox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame\n",
        "sf_crimes = (spark.read                                      #spark.read\n",
        "                  .option(\"header\", \"true\")                  #.option(<option key>, <option value>)\n",
        "                  .option(\"inferSchema\", \"true\")             #.option(<option key>, <option value>)\n",
        "                  .csv(\"San_Francisco_Crime_2018.csv\")       #.<connection_type>(<endpoint>)\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7xF15QPKEt",
        "colab_type": "code",
        "outputId": "a547a3f6-f8f6-4142-a5d5-7e8eeb383ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "sf_crimes.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|   Incident Datetime|Incident Date|Incident Time|Incident Year|Incident Day of Week|     Report Datetime|     Row ID|Incident ID|Incident Number|CAD Number|Report Type Code|Report Type Description|Filed Online|Incident Code| Incident Category|Incident Subcategory|Incident Description|          Resolution|        Intersection|     CNN|Police District|Analysis Neighborhood|Supervisor District|          Latitude|          Longitude|               point|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|2018/12/31 11:57:...|   2018/12/31|        23:57|         2018|              Monday|2019/01/01 12:02:...|75435104134|     754351|      190000015| 183653767|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|      Open or Active| 18TH ST \\ CASTRO ST|25813000|        Mission|  Castro/Upper Market|                  8| 37.76088893209152| -122.4350007026991|(37.760888932092,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433719090|     754337|      190000021| 190010038|              II|                Initial|        null|        19090|Disorderly Conduct|         Drunkenness|Alcohol, Under In...|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433704134|     754337|      190000021| 190010038|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:50:...|   2018/12/31|        23:50|         2018|              Monday|2019/01/01 04:58:...|75477706372|     754777|      196000283|      null|              II|       Coplogic Initial|        true|         6372|     Larceny Theft|Larceny Theft - O...|Theft, Other Prop...|      Open or Active|MISSION ST \\ ANNI...|24612000|       Southern| Financial Distric...|                  6|37.786851714583754|-122.40126150328634|(37.786851714584,...|\n",
            "|2018/12/31 11:30:...|   2018/12/31|        23:30|         2018|              Monday|2019/01/02 11:45:...|75517271000|     755172|      196000926|      null|              II|       Coplogic Initial|        true|        71000|     Lost Property|       Lost Property|       Lost Property|      Open or Active|VALENCIA ST \\ 22N...|24115000|        Mission|              Mission|                  8| 37.75529478092247|-122.42096409369465|(37.755294780922,...|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjva3GnDNDm3",
        "colab_type": "text"
      },
      "source": [
        "Also, you can use the following core structure for reading data: \n",
        "```\n",
        "DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n",
        "```\n",
        "The foundation for reading data in Spark is the [DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader). We access this through the **SparkSession** via the read attribute:\n",
        "\n",
        "`spark.read`\n",
        "\n",
        "After we have a DataFrame reader, we specify several values:\n",
        "- The format\n",
        "- The schema\n",
        "- The read mode\n",
        "- A series of options\n",
        "\n",
        "The *format*, *options*, and *schema* each return a DataFrameReader that can undergo further transformations and are all optional, except for one option. Each data source has a specific set of\n",
        "options that determine how the data is read into Spark. At a minimum, you must supply the DataFrameReader a path to from which to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiUlKkpBODSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame\n",
        "sf_crimes = (spark.read                                      #spark.read\n",
        "                  .format(\"csv\")                             #The format\n",
        "                  .option(\"mode\", \"FAILFAST\")                #The read mode\n",
        "                  .option(\"header\", \"true\")                  #.option(<option key>, <option value>)\n",
        "                  .option(\"inferSchema\", \"true\")             #.option(<option key>, <option value>)\n",
        "                  .load(\"San_Francisco_Crime_2018.csv\")      #path to read\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1okQMKfPXd4",
        "colab_type": "code",
        "outputId": "26f7b68b-444e-47ca-9f27-cea0ee9dabb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "sf_crimes.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|   Incident Datetime|Incident Date|Incident Time|Incident Year|Incident Day of Week|     Report Datetime|     Row ID|Incident ID|Incident Number|CAD Number|Report Type Code|Report Type Description|Filed Online|Incident Code| Incident Category|Incident Subcategory|Incident Description|          Resolution|        Intersection|     CNN|Police District|Analysis Neighborhood|Supervisor District|          Latitude|          Longitude|               point|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|2018/12/31 11:57:...|   2018/12/31|        23:57|         2018|              Monday|2019/01/01 12:02:...|75435104134|     754351|      190000015| 183653767|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|      Open or Active| 18TH ST \\ CASTRO ST|25813000|        Mission|  Castro/Upper Market|                  8| 37.76088893209152| -122.4350007026991|(37.760888932092,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433719090|     754337|      190000021| 190010038|              II|                Initial|        null|        19090|Disorderly Conduct|         Drunkenness|Alcohol, Under In...|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433704134|     754337|      190000021| 190010038|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:50:...|   2018/12/31|        23:50|         2018|              Monday|2019/01/01 04:58:...|75477706372|     754777|      196000283|      null|              II|       Coplogic Initial|        true|         6372|     Larceny Theft|Larceny Theft - O...|Theft, Other Prop...|      Open or Active|MISSION ST \\ ANNI...|24612000|       Southern| Financial Distric...|                  6|37.786851714583754|-122.40126150328634|(37.786851714584,...|\n",
            "|2018/12/31 11:30:...|   2018/12/31|        23:30|         2018|              Monday|2019/01/02 11:45:...|75517271000|     755172|      196000926|      null|              II|       Coplogic Initial|        true|        71000|     Lost Property|       Lost Property|       Lost Property|      Open or Active|VALENCIA ST \\ 22N...|24115000|        Mission|              Mission|                  8| 37.75529478092247|-122.42096409369465|(37.755294780922,...|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf59G9qhO4DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame\n",
        "sf_crimes = (spark.read                                             #spark.read\n",
        "                  .format(\"csv\")                                    #The format\n",
        "                  .option(\"mode\", \"FAILFAST\")                       #The read mode\n",
        "                  .option(\"header\", \"true\")                         #.option(<option key>, <option value>)\n",
        "                  .option(\"inferSchema\", \"true\")                    #.option(<option key>, <option value>)\n",
        "                  .option(\"path\", \"San_Francisco_Crime_2018.csv\")   #The path to read from\n",
        "                  .load()  \n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtopEQ-jPxV3",
        "colab_type": "code",
        "outputId": "58ffb952-02f3-4edd-8db5-eb32729df7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "sf_crimes.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|   Incident Datetime|Incident Date|Incident Time|Incident Year|Incident Day of Week|     Report Datetime|     Row ID|Incident ID|Incident Number|CAD Number|Report Type Code|Report Type Description|Filed Online|Incident Code| Incident Category|Incident Subcategory|Incident Description|          Resolution|        Intersection|     CNN|Police District|Analysis Neighborhood|Supervisor District|          Latitude|          Longitude|               point|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "|2018/12/31 11:57:...|   2018/12/31|        23:57|         2018|              Monday|2019/01/01 12:02:...|75435104134|     754351|      190000015| 183653767|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|      Open or Active| 18TH ST \\ CASTRO ST|25813000|        Mission|  Castro/Upper Market|                  8| 37.76088893209152| -122.4350007026991|(37.760888932092,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433719090|     754337|      190000021| 190010038|              II|                Initial|        null|        19090|Disorderly Conduct|         Drunkenness|Alcohol, Under In...|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:55:...|   2018/12/31|        23:55|         2018|              Monday|2019/01/01 12:05:...|75433704134|     754337|      190000021| 190010038|              II|                Initial|        null|         4134|           Assault|      Simple Assault|             Battery|Cite or Arrest Adult|EDDY ST \\ CYRIL M...|24893000|     Tenderloin|           Tenderloin|                  6| 37.78445272883687|-122.40849315881205|(37.784452728837,...|\n",
            "|2018/12/31 11:50:...|   2018/12/31|        23:50|         2018|              Monday|2019/01/01 04:58:...|75477706372|     754777|      196000283|      null|              II|       Coplogic Initial|        true|         6372|     Larceny Theft|Larceny Theft - O...|Theft, Other Prop...|      Open or Active|MISSION ST \\ ANNI...|24612000|       Southern| Financial Distric...|                  6|37.786851714583754|-122.40126150328634|(37.786851714584,...|\n",
            "|2018/12/31 11:30:...|   2018/12/31|        23:30|         2018|              Monday|2019/01/02 11:45:...|75517271000|     755172|      196000926|      null|              II|       Coplogic Initial|        true|        71000|     Lost Property|       Lost Property|       Lost Property|      Open or Active|VALENCIA ST \\ 22N...|24115000|        Mission|              Mission|                  8| 37.75529478092247|-122.42096409369465|(37.755294780922,...|\n",
            "+--------------------+-------------+-------------+-------------+--------------------+--------------------+-----------+-----------+---------------+----------+----------------+-----------------------+------------+-------------+------------------+--------------------+--------------------+--------------------+--------------------+--------+---------------+---------------------+-------------------+------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Dx4IzznHCW",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** - Reading the Austin Crime data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP65O-ktKVoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the Austin Crime file and save it into a Dataframe\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STZlIUqgnN76",
        "colab_type": "text"
      },
      "source": [
        "**Test your solution!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqb0UTztOtdD",
        "colab_type": "code",
        "outputId": "4af79cb4-e649-4cfb-8ee7-be9b9dbb5f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Run this cell to test your solution\n",
        "rows_austin_crimes = austin_crimes.count()\n",
        "expected = 102644\n",
        "assert str(expected) == str(rows_austin_crimes), \"{} does not equal expected {}\".format(rows_austin_crimes, expected)\n",
        "\n",
        "print(\"Tests passed\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7d74fb8527f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrows_austin_crimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maustin_crimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m102644\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_austin_crimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{} does not equal expected {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_austin_crimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tests passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'austin_crimes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft_NCSn1Rijz",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**  - Reading the Chicago crimes data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb1z9plcvkBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the Chicago crimes file and save it into a DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aM6Vwa2m9rd",
        "colab_type": "text"
      },
      "source": [
        "**Test your solution!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KqoN0CoM6ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to test your solution\n",
        "rows_chicago_crimes = chicago_crimes.count()\n",
        "expected = 266963\n",
        "assert str(expected) == str(rows_chicago_crimes), \"{} does not equal expected {}\".format(rows_chicago_crimes, expected)\n",
        "\n",
        "print(\"Tests passed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_T71tCXMLeK",
        "colab_type": "text"
      },
      "source": [
        "### Adding More Options\n",
        "\n",
        "When you import that data into a cluster, you can add options based on the specific characteristics of the data. **`option`** is a method of `DataFrameReader`. Options are key/value pairs and must be specified before calling `.csv()`. For instance, options for reading CSV data include `header`, `delimiter`, and `inferSchema`.\n",
        "\n",
        "Be aware that Spark doesn't read the header by default, so you would need to specify the `header` = True option every time you have a file with a header.\n",
        "\n",
        "**NOTE**: You can find all the available options to pass along the core data sources:\n",
        "- [CSV](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)\n",
        "- [JDBC](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.jdbc)\n",
        "- [JSON](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vej3i1fweYRj",
        "colab_type": "text"
      },
      "source": [
        "## Collecting Rows to the Driver\n",
        "There are times when you’ll want to collect some of your data to the driver in order to manipulate it on your local machine.\n",
        "Thus far, we did not explicitly define this operation. However, we used several different methods for doing so that are effectively all the same. [collect](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) gets all data from the entire DataFrame,\n",
        "[take](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) selects the first N rows, and [show](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) prints out a number of rows nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47VyAYPledTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes.take(5) # take works with an Integer count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz-DWoD-q3QY",
        "colab_type": "text"
      },
      "source": [
        "The `show` method will print by default the first 20 records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtW26Prdqflm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes.show() # this prints it out nicely"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTkgkAl8qgYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes.show(5, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJNty_UZ5Ow8",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** - collecting rows\n",
        "\n",
        "Let's try out some of this methods with the Austin and Chicago DataFrames.\n",
        "\n",
        "By default, `show()` will print the first 20 rows. If you want to print something different than 20, you can pass the number of rows you would like to print as parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y460KePa-yJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first 5 rows of the Austin Crime DataFrame\n",
        "# Collect all the Austin Crime DataFrame\n",
        "# Select the first 2 rows of the Austin Crime DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoIaP8oF4NKs",
        "colab_type": "text"
      },
      "source": [
        "**Other DataFrame methods:**\n",
        "\n",
        "\n",
        "*   **count**: Returns the number of rows in this DataFrame.\n",
        "*   **columns**: Returns all column names as a list.\n",
        "*   **dtypes**: Returns all column names and their data types as a list.\n",
        "*   **describe**: Computes basic statistics for numeric and string columns. This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.\n",
        "*  **limit**: Limits the result count to the number specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM-rMeVX5UgH",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** - using other methods\n",
        "\n",
        "Use the methods above to get the columns name and dtypes of each DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umNVubsr5zu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the columns name for sf_crimes DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6lPJukS-PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the dtypes for the austin_crimes DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMVsHz8Cng8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the count of records for the chicago_crimes DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLzW6ZYf8C4-",
        "colab_type": "text"
      },
      "source": [
        "## Applying Schemas\n",
        "\n",
        "Schemas are at the heart of data structures in Spark. A schema describes the structure of your data by naming columns and declaring the type of data in that column. Rigorously enforcing schemas leads to significant performance optimizations and reliability of code.\n",
        "\n",
        "Why is open source Spark so fast?\n",
        "* First and foremost, Spark runs first in memory rather than reading and writing to disk.\n",
        "* Second, using DataFrames allows Spark to optimize the execution of your queries because it knows what your data looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v7yABkHASBs",
        "colab_type": "text"
      },
      "source": [
        "**Schema Inference**\n",
        "\n",
        "* Import data as a DataFrame and view its schema with the `printSchema()` DataFrame method.\n",
        "* Store the schema as an object by calling `.schema` on a DataFrame. Schemas consist of a `StructType`, which is a collection of `StructField`s.  Each `StructField` gives a name and a type for a given field in the data.\n",
        "\n",
        "You can see all the available Spark types here: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdPNJfJjPrTY",
        "colab_type": "text"
      },
      "source": [
        "To display the schema of the DataFrame, use the [printSchema](httphttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchemas://) method.\n",
        "This tells you the field name, field type, and whether the column is nullable or not (default is true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXmGYfoqeWFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgxoYMkDWy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_schema = sf_crimes.schema\n",
        "print(sf_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-OeJBd0PyhY",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** - Print a Schema\n",
        "\n",
        "Take a look at the schema of the DataFrames: `austin_crimes` and `chicago_crimes`. See which fields are common between all DataFrames and which aren't.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG6vuuoPNZNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the Austin DataFrame schema\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amrGJg6gtbS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign the schema to a variable for the Austin DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFsmS_ERJ3aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the Chicago DataFrame schema\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4fsz_mmtVq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign the schema to a variable for the Chicago DataFrame\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tskyFY99BLXE",
        "colab_type": "text"
      },
      "source": [
        "Import the necessary types from the `types` module. Build a `StructType`, which takes a list of `StructField`s.  Each `StructField` takes three arguments: the name of the field, the type of data in it, and a `Boolean` for whether this field can be `Null`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gXplgDBBQOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the San Francisco DataFrame Schema\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, DoubleType, BooleanType\n",
        "             \n",
        "sf_schema = StructType([StructField(\"Incident Datetime\",StringType(),True),\n",
        "                StructField(\"Incident Date\",StringType(),True),\n",
        "                StructField(\"Incident Time\",StringType(),True),\n",
        "                StructField(\"Incident Year\",IntegerType(),True),\n",
        "                StructField(\"Incident Day of Week\",StringType(),True),\n",
        "                StructField(\"Report Datetime\",StringType(),True),\n",
        "                StructField(\"Row ID\",LongType(),True),\n",
        "                StructField(\"Incident ID\",IntegerType(),True),\n",
        "                StructField(\"Incident Number\",IntegerType(),True),\n",
        "                StructField(\"CAD Number\",IntegerType(),True),\n",
        "                StructField(\"Report Type Code\",StringType(),True),\n",
        "                StructField(\"Report Type Description\",StringType(),True),\n",
        "                StructField(\"Filed Online\",BooleanType(),True),\n",
        "                StructField(\"Incident Code\",IntegerType(),True),\n",
        "                StructField(\"Incident Category\",StringType(),True),\n",
        "                StructField(\"Incident Subcategory\",StringType(),True),\n",
        "                StructField(\"Incident Description\",StringType(),True),\n",
        "                StructField(\"Resolution\",StringType(),True),\n",
        "                StructField(\"Intersection\",StringType(),True),\n",
        "                StructField(\"CNN\",DecimalType(8,0),True),\n",
        "                StructField(\"Police District\",StringType(),True),\n",
        "                StructField(\"Analysis Neighborhood\",StringType(),True),\n",
        "                StructField(\"Supervisor District\",IntegerType(),True),\n",
        "                StructField(\"Latitude\",DoubleType(),True),\n",
        "                StructField(\"Longitude\",DoubleType(),True),\n",
        "                StructField(\"point\",StringType(),True)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMrlazAiHr3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sf_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWMOODTjB5Y8",
        "colab_type": "text"
      },
      "source": [
        "Apply the schema using the `.schema` method. This `read` returns only  the columns specified in the schema.\n",
        " A `LongType` is an 8-byte integer ranging up to 9,223,372,036,854,775,807 while `IntegerType` is a 4-byte integer ranging up to 2,147,483,647."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Q42D93B8uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the San Francisco DataFrame with a user defined schema\n",
        "sf_crimes2 = (spark.read\n",
        "                  .format(\"csv\")\n",
        "                  .schema(sf_schema)\n",
        "                  .option(\"header\", \"true\")            \n",
        "                  .load(\"San_Francisco_Crime_2018.csv\")\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJv6hxKNq8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes2.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl16HrkpA42q",
        "colab_type": "text"
      },
      "source": [
        "**Benefits of user defined schemas include**:\n",
        "\n",
        "* Avoiding the extra scan of your data needed to infer the schema\n",
        "* Providing alternative data types\n",
        "* Parsing only the fields you need (JSON files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5OByoVQPoT2",
        "colab_type": "text"
      },
      "source": [
        "**Exercise (Optional)** - Defining the Chicago crime data schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KftuHtMPuAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a schema \"chicago_schema\" for the Chicago crime DataFrame\n",
        "# Read the Chicago data using the schema you just defined\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQD8nnD1Gl5l",
        "colab_type": "text"
      },
      "source": [
        "## Handling Corrupt Records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRQJ2a8cJKA6",
        "colab_type": "text"
      },
      "source": [
        "ETL pipelines need robust solutions to handle corrupt data. This is because data corruption scales as the size of data and complexity of the data application grow. Corrupt data includes:\n",
        "\n",
        "* Missing information\n",
        "* Incomplete information\n",
        "* Schema mismatch\n",
        "* Differing formats or data types\n",
        "* User errors when writing data producers\n",
        "\n",
        "Since ETL pipelines are built to be automated, production-oriented solutions must ensure pipelines behave as expected. This means that data engineers must both expect and systematically handle corrupt records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_IYfqu7Qiib",
        "colab_type": "text"
      },
      "source": [
        "### Read modes\n",
        "Reading data from an external source naturally entails encountering malformed data, especially when working with only semi-structured data sources. Read modes specify what will happen\n",
        "when Spark does come across malformed records.\n",
        "\n",
        "![alt text](https://images-workshop.s3.amazonaws.com/Spark_read_modes.png)\n",
        "\n",
        "Source: Spark: The Definitive Guide. Big Data Processing Made Simple By Matei Zaharia, Bill Chambers\n",
        "\n",
        "The default is *permissive*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpA9skxHWY4-",
        "colab_type": "text"
      },
      "source": [
        "### Permissive mode\n",
        "Sets all fields to `null` when it encounters a corrupted record and places all corrupted records in a string column called `corrupt_record`.\n",
        "You will need to add this column to your schema if you are manually defining it.\n",
        "\n",
        "Let's see what happens when we commented out one of the schema fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3x6iZxnWW-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, DoubleType, BooleanType\n",
        "             \n",
        "sf_schema = StructType([StructField(\"Incident Datetime\",StringType(),True),\n",
        "                StructField(\"Incident Date\",StringType(),True),\n",
        "                StructField(\"Incident Time\",StringType(),True),\n",
        "                StructField(\"Incident Year\",IntegerType(),True),\n",
        "                StructField(\"Incident Day of Week\",StringType(),True),\n",
        "                StructField(\"Report Datetime\",StringType(),True),\n",
        "                StructField(\"Row ID\",LongType(),True),\n",
        "                StructField(\"Incident ID\",IntegerType(),True),\n",
        "                StructField(\"Incident Number\",IntegerType(),True),\n",
        "                # StructField(\"CAD Number\",IntegerType(),True),\n",
        "                StructField(\"Report Type Code\", StringType(),True),\n",
        "                StructField(\"Report Type Description\",StringType(),True),\n",
        "                StructField(\"Filed Online\",BooleanType(),True),\n",
        "                StructField(\"Incident Code\",IntegerType(),True),\n",
        "                StructField(\"Incident Category\",StringType(),True),\n",
        "                StructField(\"Incident Subcategory\",StringType(),True),\n",
        "                StructField(\"Incident Description\",StringType(),True),\n",
        "                StructField(\"Resolution\",StringType(),True),\n",
        "                StructField(\"Intersection\",StringType(),True),\n",
        "                StructField(\"CNN\",DecimalType(8,0),True),\n",
        "                StructField(\"Police District\",StringType(),True),\n",
        "                StructField(\"Analysis Neighborhood\",StringType(),True),\n",
        "                StructField(\"Supervisor District\",IntegerType(),True),\n",
        "                StructField(\"Latitude\",DoubleType(),True),\n",
        "                StructField(\"Longitude\",DoubleType(),True),\n",
        "                StructField(\"point\",StringType(),True),\n",
        "                StructField(\"_corrupt_record\", StringType(), True)\n",
        "                ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucqK0UHZWo7j",
        "colab_type": "text"
      },
      "source": [
        "We just ignored one of the columns in the csv file. When Sparks meets a corrupted record in a CSV, it puts the malformed string into a field configured by `columnNameOfCorruptRecord`, and sets other fields to null.\n",
        "Let's see what happen when we read the file using an user-define schema. \n",
        "\n",
        "**NOTE:** you should set a string type field named `_corrupt_record` in an user-defined schema to get the corrupt records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psWU2M2RWoIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame using permissive mode\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "sf_crimes2 = (spark.read\n",
        "                  .schema(sf_schema)\n",
        "                  .format(\"csv\")\n",
        "                  .option(\"mode\", \"permissive\")\n",
        "                  .option(\"header\", \"true\") \n",
        "                  .load(\"San_Francisco_Crime_2018.csv\")\n",
        "            )\n",
        "\n",
        "sf_crimes2.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQHW0R9ApCiZ",
        "colab_type": "text"
      },
      "source": [
        "What about if we have a file where some records have less/more fields? When it meets a record having fewer tokens than the length of the schema, sets null to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens. In both cases, the `_corrupt_record` will contain the string value of the whole row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTeMhOhlpdAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, DoubleType, BooleanType\n",
        "             \n",
        "sf_schema = StructType([StructField(\"Incident Datetime\",StringType(),True),\n",
        "                StructField(\"Incident Date\",StringType(),True),\n",
        "                StructField(\"Incident Time\",StringType(),True),\n",
        "                StructField(\"Incident Year\",IntegerType(),True),\n",
        "                StructField(\"Incident Day of Week\",StringType(),True),\n",
        "                StructField(\"Report Datetime\",StringType(),True),\n",
        "                StructField(\"Row ID\",LongType(),True),\n",
        "                StructField(\"Incident ID\",IntegerType(),True),\n",
        "                StructField(\"Incident Number\",IntegerType(),True),\n",
        "                StructField(\"CAD Number\",IntegerType(),True),\n",
        "                StructField(\"Report Type Code\", StringType(),True),\n",
        "                StructField(\"Report Type Description\",StringType(),True),\n",
        "                StructField(\"Filed Online\",BooleanType(),True),\n",
        "                StructField(\"Incident Code\",IntegerType(),True),\n",
        "                StructField(\"Incident Category\",StringType(),True),\n",
        "                StructField(\"Incident Subcategory\",StringType(),True),\n",
        "                StructField(\"Incident Description\",StringType(),True),\n",
        "                StructField(\"Resolution\",StringType(),True),\n",
        "                StructField(\"Intersection\",StringType(),True),\n",
        "                StructField(\"CNN\",DecimalType(8,0),True),\n",
        "                StructField(\"Police District\",StringType(),True),\n",
        "                StructField(\"Analysis Neighborhood\",StringType(),True),\n",
        "                StructField(\"Supervisor District\",IntegerType(),True),\n",
        "                StructField(\"Latitude\",DoubleType(),True),\n",
        "                StructField(\"Longitude\",DoubleType(),True),\n",
        "                StructField(\"point\",StringType(),True),\n",
        "                StructField(\"_corrupt_record\",StringType(), True)\n",
        "                ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thExeQUllqQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame using permissive mode with corrupted records\n",
        "sf_crimes2 = (spark.read\n",
        "                  .format(\"csv\")\n",
        "                  .schema(sf_schema)\n",
        "                  .option(\"mode\", \"permissive\")\n",
        "                  .option(\"header\", \"true\")            \n",
        "                  .load(\"San_Francisco_Crime_2018_corrupt.csv\")\n",
        "            )\n",
        "\n",
        "sf_crimes2.filter(col(\"_corrupt_record\").isNotNull()).show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxr3Z4I0IJ3Z",
        "colab_type": "text"
      },
      "source": [
        "Let's see now the record with less columns. Spark just assigned NULL to those columns (Longitude, point)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBiTEx0trQe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes2.filter((col(\"Incident Datetime\")==\"2018/12/31 11:15:00 PM\") & (col(\"Report Datetime\")==\"2019/01/01 11:26:00 PM\")).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOIUxV_XIO_4",
        "colab_type": "text"
      },
      "source": [
        "Below is the record with extra columns. See how the original record contained the last column with the value of \"Testing\" and how it has been removed from the data that was read into the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCBxwhJcnP57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes2.filter(col(\"Incident Datetime\")==\"2018/12/30 11:10:00 PM\").first()[\"_corrupt_record\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP6YSUDBgzO7",
        "colab_type": "text"
      },
      "source": [
        "### Failfast mode\n",
        "Fails immediately upon finding a corrupted record.\n",
        "\n",
        "Let's see what happens when we load a file with less records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pihAemklO-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, DoubleType, BooleanType\n",
        "\n",
        "sf_schema = StructType([StructField(\"Incident Datetime\",StringType(),True),\n",
        "                StructField(\"Incident Date\",StringType(),True),\n",
        "                StructField(\"Incident Time\",StringType(),True),\n",
        "                StructField(\"Incident Year\",IntegerType(),True),\n",
        "                StructField(\"Incident Day of Week\",StringType(),True),\n",
        "                StructField(\"Report Datetime\",StringType(),True),\n",
        "                StructField(\"Row ID\",LongType(),True),\n",
        "                StructField(\"Incident ID\",IntegerType(),True),\n",
        "                StructField(\"Incident Number\",IntegerType(),True),\n",
        "                StructField(\"CAD Number\",IntegerType(),True),\n",
        "                StructField(\"Report Type Code\", StringType(),True),\n",
        "                StructField(\"Report Type Description\",StringType(),True),\n",
        "                StructField(\"Filed Online\",BooleanType(),True),\n",
        "                StructField(\"Incident Code\",IntegerType(),True),\n",
        "                StructField(\"Incident Category\",StringType(),True),\n",
        "                StructField(\"Incident Subcategory\",StringType(),True),\n",
        "                StructField(\"Incident Description\",StringType(),True),\n",
        "                StructField(\"Resolution\",StringType(),True),\n",
        "                StructField(\"Intersection\",StringType(),True),\n",
        "                StructField(\"CNN\",DecimalType(8,0),True),\n",
        "                StructField(\"Police District\",StringType(),True),\n",
        "                StructField(\"Analysis Neighborhood\",StringType(),True),\n",
        "                StructField(\"Supervisor District\",IntegerType(),True),\n",
        "                StructField(\"Latitude\",DoubleType(),True),\n",
        "                StructField(\"Longitude\",DoubleType(),True),\n",
        "                StructField(\"point\",StringType(),True)\n",
        "                ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9ek0RA7fw3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the San Francisco DataFrame using failfast mode \n",
        "sf_crimes3 = (spark.read\n",
        "                  .schema(sf_schema)\n",
        "                  .format(\"csv\")\n",
        "                  .option(\"mode\", \"failfast\")\n",
        "                  .option(\"header\", \"true\")          \n",
        "                  .load(\"San_Francisco_Crime_2018_corrupt.csv\")\n",
        "            )\n",
        "\n",
        "sf_crimes3.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7HadLh7kXS",
        "colab_type": "text"
      },
      "source": [
        "### DropMalformed mode\n",
        "Now, let's see how our two malformed records get dropped from our corrupt DataFrame. If we read our data again using the permissive mode and we compare with our original dataFrame we can see below the total number of records we should have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tsxBMAK8KQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes2 = (spark.read\n",
        "                  .schema(sf_schema)\n",
        "                  .format(\"csv\")\n",
        "                  .option(\"mode\", \"permissive\")\n",
        "                  .option(\"header\", \"true\")            \n",
        "                  .load(\"San_Francisco_Crime_2018_corrupt.csv\")\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyE0x_xC8AHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes2.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj8hKTrJ8PcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf_crimes.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vi65Lev8fj5",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** - DropMalformed mode\n",
        "\n",
        "Try to read the file using the `dropMalformed` mode and check whether the new DataFrame has two less records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36JJqD4J73Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use DropMalformed mode to read the San Francisco crime file\n",
        "# Filename: San_Francisco_Crime_2018_corrupt.csv\n",
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3kuHOH9E29",
        "colab_type": "text"
      },
      "source": [
        "**Test your solution!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWsHPx2Q85HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to test your solution\n",
        "rows_sf_crimes2 = sf_crimes2.count()\n",
        "expected = 154530\n",
        "assert str(expected) == str(rows_sf_crimes2), \"{} does not equal expected {}\".format(rows_sf_crimes2, expected)\n",
        "\n",
        "print(\"Tests passed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm0xYgNxSaFr",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "- Download Spark: http://apache.osuosl.org/spark/\n",
        "- San Francisco Crime Dataset: https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-2018-to-Present/wg3w-h783\n",
        "- Austin Crime Dataset: https://data.austintexas.gov/Public-Safety/Crime-Reports-2018/vmn9-3bvu\n",
        "- Chicago Crime Dataset: https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2\n",
        "- Population dataset: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
        "\n",
        "\n",
        "- PySpark Documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
        "- Spark: The Definitive Guide. Big Data Processing Made Simple\n",
        "By Matei Zaharia, Bill Chambers\n",
        "Publisher: O'Reilly Media\n",
        "Release Date: February 2018\n",
        "- Mastering Apache Spark: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/\n",
        "- SP820: ETL Part 1: Data Extraction (AWS Databricks) https://academy.databricks.com/course/SP820\n",
        "- SP821: ETL Part 2: Transformations and Loads (AWS Databricks) https://academy.databricks.com/course/SP821\n",
        "- SP822: ETL Part 3: Production (AWS Databricks) https://academy.databricks.com/course/SP822"
      ]
    }
  ]
}